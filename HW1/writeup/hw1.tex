%% LyX 2.2.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[ruled]{article}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose}
\usepackage{color}
\usepackage{url}
\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=true]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
%% Special footnote code from the package 'stblftnt.sty'
%% Author: Robin Fairbairns -- Last revised Dec 13 1996
\let\SF@@footnote\footnote
\def\footnote{\ifx\protect\@typeset@protect
    \expandafter\SF@@footnote
  \else
    \expandafter\SF@gobble@opt
  \fi
}
\expandafter\def\csname SF@gobble@opt \endcsname{\@ifnextchar[%]
  \SF@gobble@twobracket
  \@gobble
}
\edef\SF@gobble@opt{\noexpand\protect
  \expandafter\noexpand\csname SF@gobble@opt \endcsname}
\def\SF@gobble@twobracket[#1]#2{}

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\makeatother

\usepackage{listings}
\lstset{backgroundcolor={\color{white}},
basicstyle={\footnotesize\ttfamily},
breakatwhitespace=false,
breaklines=true,
captionpos=b,
commentstyle={\color{mygreen}},
deletekeywords={...},
escapeinside={\%*}{*)},
extendedchars=true,
frame=shadowbox,
keepspaces=true,
keywordstyle={\color{blue}},
language=Python,
morekeywords={*,...},
numbers=none,
numbersep=5pt,
numberstyle={\tiny\color{mygray}},
rulecolor={\color{black}},
showspaces=false,
showstringspaces=false,
showtabs=false,
stepnumber=1,
stringstyle={\color{mymauve}},
tabsize=2}
\begin{document}
\global\long\def\reals{\mathbf{R}}
 \global\long\def\integers{\mathbf{Z}}
\global\long\def\naturals{\mathbf{N}}
 \global\long\def\rationals{\mathbf{Q}}
\global\long\def\ca{\mathcal{A}}
\global\long\def\cb{\mathcal{B}}
 \global\long\def\cc{\mathcal{C}}
 \global\long\def\cd{\mathcal{D}}
\global\long\def\ce{\mathcal{E}}
\global\long\def\cf{\mathcal{F}}
\global\long\def\cg{\mathcal{G}}
\global\long\def\ch{\mathcal{H}}
\global\long\def\ci{\mathcal{I}}
\global\long\def\cj{\mathcal{J}}
\global\long\def\ck{\mathcal{K}}
\global\long\def\cl{\mathcal{L}}
\global\long\def\cm{\mathcal{M}}
\global\long\def\cn{\mathcal{N}}
\global\long\def\co{\mathcal{O}}
\global\long\def\cp{\mathcal{P}}
\global\long\def\cq{\mathcal{Q}}
\global\long\def\calr{\mathcal{R}}
\global\long\def\cs{\mathcal{S}}
\global\long\def\ct{\mathcal{T}}
\global\long\def\cu{\mathcal{U}}
\global\long\def\cv{\mathcal{V}}
\global\long\def\cw{\mathcal{W}}
\global\long\def\cx{\mathcal{X}}
\global\long\def\cy{\mathcal{Y}}
\global\long\def\cz{\mathcal{Z}}
\global\long\def\ind#1{1(#1)}
\global\long\def\pr{\mathbb{P}}

\global\long\def\ex{\mathbb{E}}
\global\long\def\var{\textrm{Var}}
\global\long\def\cov{\textrm{Cov}}
\global\long\def\sgn{\textrm{sgn}}
\global\long\def\sign{\textrm{sign}}
\global\long\def\kl{\textrm{KL}}
\global\long\def\law{\mathcal{L}}
\global\long\def\eps{\varepsilon}
\global\long\def\convd{\stackrel{d}{\to}}
\global\long\def\eqd{\stackrel{d}{=}}
\global\long\def\del{\nabla}
\global\long\def\loss{\ell}
\global\long\def\tr{\operatorname{tr}}
\global\long\def\trace{\operatorname{trace}}
\global\long\def\diag{\text{diag}}
\global\long\def\rank{\text{rank}}
\global\long\def\linspan{\text{span}}
\global\long\def\proj{\text{Proj}}
\global\long\def\argmax{\operatornamewithlimits{arg\, max}}
\global\long\def\argmin{\operatornamewithlimits{arg\, min}}
\global\long\def\bfx{\mathbf{x}}
\global\long\def\bfy{\mathbf{y}}
\global\long\def\bfl{\mathbf{\lambda}}
\global\long\def\bfm{\mathbf{\mu}}
\global\long\def\calL{\mathcal{L}}
\global\long\def\vw{\boldsymbol{w}}
\global\long\def\vx{\boldsymbol{x}}
\global\long\def\vxi{\boldsymbol{\xi}}
\global\long\def\valpha{\boldsymbol{\alpha}}
\global\long\def\vbeta{\boldsymbol{\beta}}
\global\long\def\vsigma{\boldsymbol{\sigma}}
\global\long\def\vmu{\boldsymbol{\mu}}
\global\long\def\vtheta{\boldsymbol{\theta}}
\global\long\def\vd{\boldsymbol{d}}
\global\long\def\vs{\boldsymbol{s}}
\global\long\def\vt{\boldsymbol{t}}
\global\long\def\vh{\boldsymbol{h}}
\global\long\def\ve{\boldsymbol{e}}
\global\long\def\vf{\boldsymbol{f}}
\global\long\def\vg{\boldsymbol{g}}
\global\long\def\vz{\boldsymbol{z}}
\global\long\def\vk{\boldsymbol{k}}
\global\long\def\va{\boldsymbol{a}}
\global\long\def\vb{\boldsymbol{b}}
\global\long\def\vv{\boldsymbol{v}}
\global\long\def\vy{\boldsymbol{y}}

\title{Machine Learning and Computational Statistics\\
Homework 1: Ridge Regression, Gradient Descent, and SGD}

\maketitle
\textbf{Instructions}: Your answers to the questions below, including
plots and mathematical work, should be submitted as a single PDF file.
It's preferred that you write your answers using software that typesets
mathematics (e.g. \LaTeX{}, \LyX{}, or MathJax via iPython), though
if you need to you may scan handwritten work. You may find the \href{https://github.com/gpoore/minted}{minted}
package convenient for including source code in your \LaTeX{} document.
If you are using \LyX{}, then the \href{https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings}{listings}
package tends to work better.


\section{Introduction}

In this homework you will implement ridge regression using gradient
descent and stochastic gradient descent. We've provided a lot of support
Python code to get you started on the right track. References below
to particular functions that you should modify are referring to the
support code, which you can download from the website. If you have
time after completing the assignment, you might pursue some of the
following:
\begin{itemize}
\item Study up on numpy's \href{https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html}{broadcasting}
to see if you can simplify and/or speed up your code.
\item Think about how you could make the code more modular so that you could
easily try different loss functions and step size methods. 
\item Experiment with more sophisticated approaches to setting the step
sizes for SGD (e.g. try out the recommendations in ``Bottou's SGD
Tricks'' on the website) 
\item Instead of taking 1 data point at a time, as in SGD, try minibatch
gradient descent, where you use multiple points at a time to get your
step direction. How does this effect convergence speed? Are you getting
computational speedup as well by using vectorized code?
\item Advanced: What kind of loss function will give us ``quantile regression''?
\end{itemize}

\section{Linear Regression}

\subsection{Feature Normalization}

When feature values differ greatly, we can get much slower rates of
convergence of gradient-based algorithms. Furthermore, when we start
using regularization (introduced in a later problem), features with
larger values are treated as ``more important'', which is not usually
what you want.  One common approach to feature normalization is perform
an affine transformation (i.e. shift and rescale) on each feature
so that all feature values in the training set are in $[0,1]$. Each
feature gets its own transformation. We then apply the same transformations
to each feature on the test\footnote{Throughout this assignment we refer to the ``test'' set. It may
be more appropriate to call this set the ``validation'' set, as
it will be a set of data on which we compare the performance of multiple
models. Typically a test set is only used once, to assess the performance
of the model that performed best on the validation set.} set. It's important that the transformation is ``learned'' on the
training set, and then applied to the test set. It is possible that
some transformed test set values will lie outside the $[0,1]$ interval.

Modify function \texttt{feature\_normalization} to normalize all the
features to $[0,1]$. (Can you use numpy's ``broadcasting'' here?)
 Note that a feature with constant value cannot be normalized in
this way. Your function should discard features that are constant
in the training set.


\subsection{Gradient Descent Setup}

In linear regression, we consider the hypothesis space of linear functions
$h_{\theta}:\reals^{d}\to\reals$, where
\[
h_{\theta}(x)=\theta^{T}x,
\]
for $\theta,x\in\reals^{d}$, and we choose $\theta$ that minimizes
the following ``square loss'' objective function: 
\[
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x_{i})-y_{i}\right)^{2},
\]
where $(x_{1},y_{1}),\ldots,(x_{m},y_{m})\in\reals^{d}\times\reals$
is our training data.

While this formulation of linear regression is very convenient, it's
more standard to use a hypothesis space of ``affine'' functions:
\[
h_{\theta,b}(x)=\theta^{T}x+b,
\]
which allows a ``bias'' or nonzero intercept term. The standard
way to achieve this, while still maintaining the convenience of the
first representation, is to add an extra dimension to $x$ that is
always a fixed value, such as 1. You should convince yourself that
this is equivalent. We'll assume this representation, and thus we'll
actually take $\theta,x\in\reals^{d+1}$.
\begin{enumerate}
\item Let $X\in\reals^{m\times\left(d+1\right)}$ be the \textbf{design
matrix}, where the $i$'th row of $X$ is $x_{i}$. Let $y=\left(y_{1},\ldots,y_{m}\right)^{T}\in\reals^{m\times1}$
be the ``response''. Write the objective function $J(\theta)$ as
a matrix/vector expression, without using an explicit summation sign.
{[}Being able to write expressions as matrix/vector expressions without
summations is crucial to making implementations that are useful in
practice, since you can use numpy (or more generally, an efficient
numerical linear algebra library) to implement these matrix/vector
operations orders of magnitude faster than naively implementing with
loops in Python.{]} 
\item Write down an expression for the gradient of $J$ (again, as a matrix/vector
expression, without using an explicit summation sign). 
\item In our search for a $\theta$ that minimizes $J$, suppose we take
a step from $\theta$ to $\theta+\eta h$, where $h\in\reals^{d+1}$
is the ``step direction'' (recall, this is not necessarily a unit
vector) and $\eta\in(0,\infty)$ is the ``step size'' (note that
this is not the actual length of the step, which is $\eta\|h\|$).
Use the gradient to write down an approximate expression for the change
in objective function value $J(\theta+\eta h)-J(\theta)$. {[}This
approximation is called a ``linear'' or ``first-order'' approximation.{]}
\item Write down the expression for updating $\theta$ in the gradient descent
algorithm. Let $\eta$ be the step size.
\item Modify the function \texttt{compute\_square\_loss}, to compute $J(\theta)$
for a given $\theta$. You might want to create a small dataset for
which you can compute $J(\theta)$ by hand, and verify that your \texttt{compute\_square\_loss}
function returns the correct value.
\item Modify the function \texttt{compute\_square\_loss\_gradient}, to compute
$\del_{\theta}J(\theta)$. You may again want to use a small dataset
to verify that your \texttt{compute\_square\_loss\_gradient} function
returns the correct value.
\end{enumerate}

\subsection{(OPTIONAL) Gradient Checker}

\noindent For many optimization problems, coding up the gradient correctly
can be tricky. Luckily, there is a nice way to numerically check the
gradient calculation. If $J:\reals^{d}\to\reals$ is differentiable,
then for any vector $h\in\reals^{d}$, the directional derivative
of $J$ at $\theta$ in the direction $h$ is given by\footnote{Of course, it is also given by the more standard definition of directional
derivative, $\lim_{\eps\to0}\frac{1}{\eps}\left[J(\theta+\eps h)-J(\theta)\right]$.
The form given gives a better approximation to the derivative when
we are using small (but not infinitesimally small) $\eps$.}
\[
\lim_{\eps\to0}\frac{J(\theta+\eps h)-J(\theta-\eps h)}{2\epsilon}.
\]
We can approximate this directional derivative by choosing a small
value of $\eps>0$ and evaluating the quotient above. We can get an
approximation to the gradient by approximating the directional derivatives
in each coordinate direction and putting them together into a vector.
In other words, take $h=\left(1,0,0,\ldots,0\right)$ to get the first
component of the gradient. Then take $h=(0,1,0,\ldots,0)$ to get
the second component. And so on. See \url{http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization}
for details. 
\begin{enumerate}
\item Complete the function \texttt{grad\_checker} according to the documentation
given. Alternatively, you may complete the function \texttt{generic\_grad\_checker
so} that it works for any objective function. It should take as parameters
a function that computes the objective function and a function that
computes the gradient of the objective function. Note: Running the
gradient checker takes extra time. In practice, once you're convinced
your gradient calculator is correct, you should stop calling the checker
so things run faster. 
\end{enumerate}

\subsection{Batch Gradient Descent\protect\footnote{Sometimes people say ``batch gradient descent'' or ``full batch
gradient descent'' to mean gradient descent, defined as we discussed
in class. They do this to distinguish it from stochastic gradient
descent and minibatch gradient descent, which they probably use as
their default.}}

At the end of the skeleton code, the data is loaded, split into a
training and test set, and normalized. We'll now finish the job of
running regression on the training set. Later on we'll plot the results
together with SGD results.
\begin{enumerate}
\item Complete \texttt{batch\_gradient\_descent}. 
\item Now let's experiment with the step size. Note that if the step size
is too large, gradient descent may not converge\footnote{For the mathematically inclined, there is a theorem that if the objective
function is convex and differentiable, and the gradient of the objective
is Lipschitz continuous with constant $L>0$, then gradient descent
converges for fixed steps of size $1/L$ or smaller. See \url{https://www.cs.cmu.edu/~ggordon/10725-F12/scribes/10725_Lecture5.pdf},
Theorem 5.1.}. Starting with a step-size of $0.1$, try various different fixed
step sizes to see which converges most quickly and/or which diverge.
As a minimum, try step sizes 0.5, 0.1, .05, and .01. Plot the value
of the objective function as a function of the number of steps for
each step size. Briefly summarize your findings. 
\item (Optional) Implement backtracking line search (google it). How does
it compare to the best fixed step-size you found in terms of number
of steps? In terms of time? How does the extra time to run backtracking
line search at each step compare to the time it takes to compute the
gradient? (You can also compare the operation counts.)
\end{enumerate}

\subsection{Ridge Regression (i.e. Linear Regression with $\ell_{2}$ regularization)}

When we have a large number of features compared to instances, regularization
can help control overfitting. Ridge regression is linear regression
with $\ell_{2}$ regularization. The regularization term is sometimes
called a penalty term. The objective function for ridge regression
is
\[
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x_{i})-y_{i}\right)^{2}+\lambda\theta^{T}\theta,
\]
where $\lambda$ is the regularization parameter, which controls the
degree of regularization. Note that the bias parameter is being regularized
as well. We will address that below.
\begin{enumerate}
\item Compute the gradient of $J(\theta)$ and write down the expression
for updating $\theta$ in the gradient descent algorithm. (Matrix/vector
expression \textendash{} no summations please.)
\item Implement \texttt{compute\_regularized\_square\_loss\_gradient.}
\item Implement \texttt{regularized\_grad\_descent.}
\item For regression problems, we may prefer to leave the bias term unregularized.
One approach is to change $J(\theta)$ so that the bias is separated
out from the other parameters and left unregularized. Another approach
that can achieve approximately the same thing is to use a very large
number $B$, rather than $1$, for the extra bias dimension. Explain
why making $B$ large decreases the effective regularization on the
bias term, and how we can make that regularization as weak as we like
(though not zero).
\item (Optional) Develop a formal statement of the claim in the previous
problem, and prove the statement. \\
\item (Optional) Try various values of $B$ to see what performs best in
test.
\item Now fix $B=1$. Choosing a reasonable step-size (or using backtracking
line search), find the $\theta_{\lambda}^{*}$ that minimizes $J(\theta)$
over a range of $\lambda$. You should plot the training loss and
the test loss (just the square loss part, without the regularization,
in each case) as a function of $\lambda$. Your goal is to find $\lambda$
that gives the minimum test loss. It's hard to predict what $\lambda$
that will be, so you should start your search very broadly, looking
over several orders of magnitude. For example, $\lambda\in\left\{ 10^{-7},10^{-5},10^{-3},10^{-1},1,10,100\right\} $.
Once you find a range that works better, keep zooming in. You may
want to have $\log(\lambda)$ on the $x$-axis rather than $\lambda$.
{[}If you like, you may use sklearn to help with the hyperparameter
search.{]} 
\item What $\theta$ would you select for deployment and why?
\end{enumerate}

\subsection{Stochastic Gradient Descent}

\noindent When the training data set is very large, evaluating the
gradient of the objective function can take a long time, since it
requires looking at each training example to take a single gradient
step. When the objective function takes the form of an average of
many values, such as
\[
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}f_{i}(\theta)
\]
 (as it does in the empirical risk), stochastic gradient descent (SGD)
can be very effective. In SGD, rather than taking $-\del J(\theta)$
as our step direction, we take $-\del f_{i}(\theta)$ for some $i$
chosen uniformly at random from $\{1,\ldots,m\}$. The approximation
is poor, but we will show it is unbiased. 

\noindent In machine learning applications, each $f_{i}(\theta)$
would be the loss on the $i$th example (and of course we'd typically
write $n$ instead of $m$, for the number of training points). In
practical implementations for ML, the data points are \textbf{randomly
shuffled}, and then we sweep through the whole training set one by
one, and perform an update for each training example individually.
One pass through the data is called an \textbf{epoch}. Note that each
epoch of SGD touches as much data as a single step of batch gradient
descent. You can use the same ordering for each epoch, though optionally
you could investigate whether reshuffling after each epoch affects
the convergence speed. 
\begin{enumerate}
\item Show that the objective function 
\[
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x_{i})-y_{i}\right)^{2}+\lambda\theta^{T}\theta
\]
can be written in the form $J(\theta)=\frac{1}{m}\sum_{i=1}^{m}f_{i}(\theta)$
by giving an expression for $f_{i}(\theta)$ that makes the two expressions
equivalent.
\item Show that the stochastic gradient $\del f_{i}(\theta)$, for $i$
chosen uniformly at random from $\{1,\ldots,m\}$, is an \textbf{unbiased
estimator} of $\del J(\theta)$. In other words, show that $\ex\left[\del f_{i}(\theta)\right]=\del J(\theta)$
for any $\theta$. (Hint: It will be easier, notationally, to prove
this for a general $J(\theta)=\frac{1}{m}\sum_{i=1}^{m}f_{i}(\theta)$,
rather than the specific case of ridge regression. You can start by
writing down an expression for $\ex\left[\del f_{i}(\theta)\right]$...)
\item \emph{W}rite down the update rule for $\theta$ in SGD for the ridge
regression objective function.
\item Implement \texttt{stochastic\_grad\_descent}. (Note: You could potentially
generalize the code you wrote for batch gradient to handle minibatches
of any size, including 1, but this is not necessary.)
\item Use SGD to find $\theta_{\lambda}^{*}$ that minimizes the ridge regression
objective for the $\lambda$ and $B$ that you selected in the previous
problem. (If you could not solve the previous problem, choose $\lambda=10^{-2}$
and $B=1$). Try a few fixed step sizes (at least try $\eta_{t}\in\left\{ 0.05,.005\right\} $.
Note that SGD may not converge with fixed step size. Simply note your
results. Next try step sizes that decrease with the step number according
to the following schedules: $\eta_{t}=\frac{1}{t}$ and $\eta_{t}=\frac{1}{\sqrt{t}}$.
For each step size rule, plot the value of the objective function
(or the log of the objective function if that is more clear) as a
function of epoch (or step number, if you prefer) for each of the
approaches to step size. How do the results compare? Some things to
note: 1) In this case we are investigating the convergence rate of
the optimization algorithm with different step size schedules, thus
we're interested in the value of the objective function, which includes
the regularization term. 2) Sometimes the initial step size ($1$
for $1/t$ and $1/\sqrt{t}$) is too aggressive and will get you into
a part of parameter space from which you can't recover. Although in
this problem, this probably means you implemented something poorly,
in general you can try starting at $t=10$, for example, to skip the
large initial steps. 3) As we'll learn in an upcoming lecture, SGD
convergence is much slower than GD once we get close to the minimizer.
(Remember, the SGD step directions are very noisy versions of the
GD step direction). If you look at the objective function values on
a logarithmic scale, it may look like SGD will never find objective
values that are as low as GD gets. In terminology we'll learn in Lecture
2, GD has much smaller ``optimization error'' than SGD. However,
this difference in optimization error is usually dominated by other
sources of error (estimation error and approximation error). Moreover,
for very large datasets, SGD (or minibatch GD) is much faster (by
wall-clock time) than GD to reach a point that's close {[}enough{]}
to the minimizer. 
\item (Optional) Try a stepsize rule of the form $\eta_{t}=\frac{\eta_{0}}{1+\eta_{0}\lambda t}$,
where $\lambda$ is your regularization constant, and $\eta_{0}$
a constant you can choose. How do the results compare?
\end{enumerate}

\section{Risk Minimization}

\subsection{Square Loss}

\global\long\def\E{\ex}
\begin{enumerate}
\item Let $y$ be a random variable with a known distribution, and consider
the square loss function $\ell(a,y)=(a-y)^{2}$. We want to find the
action $a^{*}$ that has minimal risk. That is, we want to find $a^{*}=\argmin_{a}\ex\left(a-y\right)^{2}$,
where the expectation is with respect to $y$. Show that $a^{*}=\ex y$,
and the Bayes risk (i.e. the risk of $a^{*}$) is $\var(y)$. In other
words, if you want to try to predict the value of a random variable,
the best you can do (for minimizing expected square loss) is to predict
the mean of the distribution. Your expected loss for predicting the
mean will be the variance of the distribution. {[}Hint: Recall that
$\var(y)=\ex y^{2}-\left(\ex y\right)^{2}$.{]}
\item Now let's introduce an input. Recall that the \textbf{expected loss
}or \textbf{``risk''}\emph{ }of a decision function $f:\cx\to\ca$
is
\[
R(f)=\ex\loss(f(x),y),
\]
where $(x,y)\sim P_{\cx\times\cy}$, and the \textbf{Bayes decision
function} $f^{*}:\cx\to\ca$ is a function that achieves the \emph{minimal
risk} among all possible functions: 
\[
R(f^{*})=\inf_{f}R(f).
\]
Here we consider the regression setting, in which $\ca=\cy=\reals$.
We will show for the square loss $\ell(a,y)=\left(a-y\right)^{2}$,
the Bayes decision function is $f^{*}(x)=\ex\left[y\mid x\right]$,
where the expectation is over $y$. As before, we assume we know the
data-generating distribution $P_{\cx\times\cy}$.
\begin{enumerate}
\item We'll approach this problem by finding the optimal action for any
given $x$. If somebody tells us $x$, we know that the corresponding
$y$ is coming from the conditional distribution $y\mid x$. For a
particular $x$, what value should we predict (i.e. what action $a$
should we produce) that has minimal expected loss? Express your answer
as a decision function $f(x)$, which gives the best action for any
given $x$. In mathematical notation, we're looking for $f^{*}(x)=\argmin_{a}\ex\left[\left(a-y\right)^{2}\mid x\right]$,
where the expectation is with respect to $y$. (Hint: There is really
nothing to do here except write down the answer, based on the previous
question. But make sure you understand what's happening...)
\item In the previous problem we produced a decision function $f^{*}(x)$
that minimized the risk for each $x$. In other words, for any other
decision function $f(x)$, $f^{*}(x)$ is going to be at least as
good as $f(x)$, for every single $x$. In math, we mean
\[
\ex\left[\left(f^{*}(x)-y\right)^{2}\mid x\right]\le\ex\left[\left(f(x)-y\right)^{2}\mid x\right],
\]
for all $x$. To show that $f^{*}(x)$ is the Bayes decision function,
we need to show that 
\[
\ex\left[\left(f^{*}(x)-y\right)^{2}\right]\le\ex\left[\left(f(x)-y\right)^{2}\right]
\]
for any $f$. Explain why this is true. (Hint: Law of iterated expectations.)
\end{enumerate}
\end{enumerate}

\subsection{{[}Optional{]} Median Loss}
\begin{enumerate}
\item (Optional) Show that for the absolute loss $\ell(\hat{y},y)=\left|y-\hat{y}\right|$,
$f^{*}(x)$ is a Bayes decision function if $f^{*}(x)$ is the median
of the conditional distribution of $y$ given $x$. {[}Hint: As in
the previous section, consider one $x$ at time. It may help to use
the following characterization of a median: $m$ is a median of the
distribution for random variable $y$ if $\pr(y\ge m)\ge\frac{1}{2}$
and $\pr(y\le m)\ge\frac{1}{2}$.{]} Note: This loss function leads
to ``median regression''. There are other loss functions that lead
to ``quantile regression'' for any chosen quantile. (For partial
credit, you may assume that the distribution of $y\mid x$ is discrete
or continuous. For full credit, no assumptions about the distribution.)
\end{enumerate}

\end{document}
